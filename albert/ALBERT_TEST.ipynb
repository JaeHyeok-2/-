{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from ALBERT import ALBERT\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "input_ids = torch.randint(0,29999,(3,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "====================================================================================================\n",
      "tensor([[ 2937, 24934,  3147, 20548,  7542,  1131, 28324, 21396, 12708, 11377],\n",
      "        [14902, 25179, 26533, 24678,   180,  8850, 23042, 19401, 19650,  8415],\n",
      "        [ 3691, 10781, 23233,  2325, 18618,   521, 11853, 18761, 12247, 28269]],\n",
      "       device='mps:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], device='mps:0', dtype=torch.int32)\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids.to(device)\n",
    "\n",
    "segment_ids = torch.IntTensor([[1]*10,[1]*5 +[2]*5, [2]*10]).to(device)\n",
    "input_mask = torch.BoolTensor([[0]*10, [0]*5 +[1]*5, [1] * 10]).to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(segment_ids.shape)\n",
    "print(input_mask.shape)\n",
    "print(\"==\"*50)\n",
    "print(input_ids)\n",
    "print(segment_ids)\n",
    "print(input_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "albert_test = ALBERT(vocab_size=30000)\n",
    "albert_test.to(device)\n",
    "result = albert_test(input_ids,segment_ids,mask=input_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 2.4671,  0.1909, -0.0435,  ...,  0.3825,  0.8484, -0.2222],\n         [ 0.0000,  0.1984,  0.7001,  ...,  0.2878,  1.6756, -0.3367],\n         [ 0.6444,  1.0998,  0.1340,  ...,  1.2334,  0.6162, -0.0000],\n         ...,\n         [ 1.0147, -0.3360,  0.9690,  ...,  0.9040,  1.6471, -0.2211],\n         [ 1.7698,  0.6789, -0.3571,  ...,  1.1214,  1.2900, -0.3859],\n         [ 2.3347,  1.1737,  0.0000,  ...,  1.2916,  0.0000, -0.0877]],\n\n        [[ 0.8791, -1.1332,  0.5328,  ...,  1.1513,  1.1676, -0.1885],\n         [ 2.1402, -0.5334,  0.0000,  ...,  0.0758,  1.8289,  0.1297],\n         [ 0.6398, -0.8256,  0.0000,  ...,  0.6134,  3.8966,  0.7533],\n         ...,\n         [ 1.3614, -0.2157,  0.0049,  ...,  0.3635,  1.6792,  1.1353],\n         [ 1.4331, -1.0871,  0.5986,  ...,  0.1337,  2.5710,  0.2612],\n         [ 0.7978,  0.3364,  1.1188,  ...,  0.1144,  2.6503,  0.3094]],\n\n        [[ 3.5548,  0.4170, -0.1092,  ..., -0.2692,  1.7388, -0.7333],\n         [ 2.7976,  0.8087,  0.7484,  ...,  0.1960,  0.7131, -0.5258],\n         [ 3.6709,  0.5571, -1.0558,  ..., -0.2938,  0.0000, -0.1459],\n         ...,\n         [ 2.2956, -0.2352, -0.6698,  ...,  1.1384,  1.1646, -0.8036],\n         [ 4.1022, -0.1229, -0.0955,  ..., -0.0000,  2.0451, -0.4897],\n         [ 0.8847,  0.5407,  0.1957,  ...,  0.9391,  1.3724,  0.0090]]],\n       device='mps:0', grad_fn=<SliceBackward0>)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jh/opt/anaconda3/envs/M1GPU/lib/python3.8/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m2.\u001B[39m],requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      3\u001B[0m y \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([2.],requires_grad=True).to(device)\n",
    "y = x ** 2\n",
    "print(y.grad.save)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn((1,1,4,4))\n",
    "print(input.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "ConvTranspose2d(16, 33, kernel_size=(3, 3), stride=(2, 2))"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.ConvTranspose2d(16,33,3,stride=2)\n",
    "m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
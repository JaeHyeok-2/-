{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def spatial_pyramid_pool(self,previous_conv,num_sample,previous_conv_size,out_pool_size):\n",
    "    \"\"\"\n",
    "    :param previous_conv: a Tensor Vector of Previous Convolution Layer\n",
    "    :param num_sample: an Int Number of Image in the batch\n",
    "    :param previous_conv_size: an Int Vector [height,width] of the matrix features size of previous convolution layer\n",
    "    :param out_pool_size: a Int Vector of Expected output size of max pooling layer\n",
    "\n",
    "    :return: a tensor vector with shape [1xn] is the concentration of multi -level pooling\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(out_pool_size)) :  # Paper : 6,3,2,1\n",
    "        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))\n",
    "        w_wid = int(math.ceil(previous_conv_size[1]/ out_pool_size[i]))\n",
    "        h_pad = (h_wid * out_pool_size[i] - previous_conv_size[0] + 1) /2\n",
    "        w_pad = (w_wid * out_pool_size[i] - previous_conv_size[0] + 1) /2\n",
    "        maxpool = nn.Maxpool2d((h_wid,w_wid),stride =(h_wid,w_wid),padding=(h_pad,w_pad))\n",
    "\n",
    "        x = maxpool(previous_conv)\n",
    "\n",
    "        if i == 0 :\n",
    "            spp = x.view(num_sample,-1)\n",
    "        else :\n",
    "            spp = torch.cat((spp,x.view(num_sample)),1)\n",
    "    return spp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SPP_NET(nn.Module):\n",
    "    def __init__(self,opt,input_nc,ndf=64):\n",
    "        \"\"\"\n",
    "        CNN model which adds spp layer so that we can input multi-size tensor\n",
    "        \"\"\"\n",
    "        super(SPP_NET,self).__init__()\n",
    "        self.output_num = [4,2,1]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_nc,ndf,4,2,1,bias =False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(ndf,ndf*2,4,1,1,bias=False)\n",
    "        self.BN1 = nn.BatchNorm2d(ndf *2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(ndf*2,ndf*4,4,1,1,bias=False)\n",
    "        self.BN2 = nn.BatchNorm2d(ndf*4)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(ndf*4,ndf *8,4,1,1,bias=False)\n",
    "        self.BN3 = nn.BatchNorm2d(ndf*8)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(ndf*8,64,4,1,0,bias=False)\n",
    "        self.fc1 = nn.Linear(10752,4096)\n",
    "        self.fc2 = nn.Linear(4096,1000)\n",
    "\n",
    "\n",
    "    def forward(self,x) :\n",
    "        x = self.conv1(x)\n",
    "        x = self.LReLU1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(self.BN1(x))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(self.BN2(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        spp = spatial_pyramid_pool(x,1,[int(x.size(2)),int(x.size(3))],self.output_num)\n",
    "        fc1 = self.fc1(spp)\n",
    "        fc2 = self.fc2(fc1)\n",
    "        s = nn.Sigmoid()\n",
    "        output = s(fc2)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}